---
title: "Regression Analysis of Used Car Prices"
author: "Andy Pang"
output:
  html_document:
    toc: true
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F,cache = F)
```

<br/>

### Introduction

See the first part of this project at https://apang782.github.io/vroom1/

Here, scrapped data from vroom.com is used to create regression models -- predicting used car prices. A large amount of possible predictor variables are tested and examined for inclusion in our models.

A good understanding of the importance of whether horsepower, fuel economy, etc influence prices is invaluable to both buyers and sellers of used cars.
We aim to choose the best model/methodology of predicting car prices and to determine the relative importance of our variables.

The dataset used contains dummy variables, which indicate different types of vehicles through setting each level of the categorical variable as a column.
The baselines of these dummy variables have been taken as the following:

- Body: SUV
- Interior Color: Black
- Transmission: Automatic
- Drivetrain: FWD
- Fuel: Gasoline

To clarify, we indicate an electric vehicle for example, by having zeroes in the columns for flex fuel and diesel, but the column for electric is '1'. At the same time, to indicate a vehicle is an SUV, all columns dealing with body type are zero because our 'base' vehicle is understood as an SUV. Our categorical columns are meant to show the effects of difference from our baseline.

</br>

### Conclusions

This project demonstrated the differences in both purpose and performance of various regression methods. We first see that the removal of outliers/high influence points do not necessarily improve model performance. From the stepwise and ridge regression models, we see that having more predictors does lead to lower error but can cause overfitting. 
Furthermore, the similarities between our OLS model and the LASSO regression model are noted, as well as LASSO's improved performance.

At the same time, the power aggregate techniques is seen in their smaller MSE's at the cost of simple/explainable structure.

From multiple (simple structure) models, we see that torque, mileage, and horsepower as being the most important factors in determining car price.

Random forests provided the best predictive power in this analysis -- giving predictions that are off by around $2400.

For future consideration -- new samples or bootstrapping (taking multiple samplings with replacement during modeling) can yield more accurate and reliable results in our nonensemble methods. This can overcome the possible internal validation bias present in our model and prevent overfitted models.

<br/>

### Reading in Data

``` {r, echo = F}
# Read in data
cars<- read.csv("C:/Users/A/Desktop/carproject/cars.csv",header=TRUE,sep=",")
```


``` {r}
library(car)
set.seed(1)
train.row <- sample(1:nrow(cars),nrow(cars)*(3/4),replace=FALSE)
train <- cars[train.row,]
test <- cars[-train.row,]
#dim(train) # 651 35
#dim(test) # 218 35
```

The data is read in via a CSV file -- exported from earlier Python code. A 75/25 split is made in the data, leaving most of our observations for model training and a quarter for testing purposes. The dimensions of each set are noted.

<br/>

### Linear Regression Modeling -- Variable Selection

```{r,echo=F}
model <- lm(price~.,train)
#summary(model)
#fuel_Diesel and body_Pickup.Truck are linearly related var's
#column of zeroes

train1 <- subset(train,select=-c(fuel_Diesel,body_Pickup.Truck ))
model1 <- lm(price~.,train1)
#alias(model1)
#vif(model1)
#highest VIF is cmpg

train2 <- subset(train1,select=-cmpg)
model2 <- lm(price~.,train2)
#vif(model2) #ftrackw

train3 <- subset(train2,select=-ftrackw)
model3 <- lm(price~.,train3)
#vif(model3) #hmpg

train4 <- subset(train3,select=-hmpg)
model4 <- lm(price~.,train4)
#vif(model4) #length

train5 <- subset(train4,select=-length)
model5 <- lm(price~.,train5)
#vif(model5) #highest is height @ 9.5
#summary(model5) #interior_Tan

train6 <- subset(train5,select=-interior_Tan)
model6 <- lm(price~.,train6)
#summary(model6) #body_Hatchback

train7 <- subset(train6,select=-body_Hatchback)
model7 <- lm(price~.,train7)
#summary(model7) #body_Sedan

train8 <- subset(train7,select=-body_Sedan)
model8 <- lm(price~.,train8)
#summary(model8) #interior_White

train9 <- subset(train8,select=-interior_White)
model9 <- lm(price~.,train9)
#summary(model9) #body_Van.Minivan

train10 <- subset(train9,select=-body_Van.Minivan)
model10 <- lm(price~.,train10)
#summary(model10) #interior_Red

train11 <- subset(train10,select=-interior_Red)
model11 <- lm(price~.,train11)
#summary(model11) #tm_Other

train12 <- subset(train11,select=-tm_Other)
model12 <- lm(price~.,train12)
#summary(model12) #interior_Gray

train13 <- subset(train12,select=-interior_Gray)
model13 <- lm(price~.,train13)
#summary(model13) #owners

train14 <- subset(train13,select=-owners)
model14 <- lm(price~.,train14)
#summary(model14) #fuel_Electric

train15 <- subset(train14,select=-fuel_Electric)
model15 <- lm(price~.,train15)
#summary(model15) #height

train16 <- subset(train15,select=-height)
model16 <- lm(price~.,train16)
#summary(model16) #tm_Manual

train17 <- subset(train16,select=-tm_Manual)
model17 <- lm(price~.,train17)
#summary(model17) #year

train18 <- subset(train17,select=-year)
model18 <- lm(price~.,train18)
#summary(model18) #fuel_Gas.Electric.Hybrid

train19 <- subset(train18,select=-fuel_Gas.Electric.Hybrid)
model19 <- lm(price~.,train19)
#summary(model19) #looks good
#vif(model19) #only hp, wheelb, and torque have VIFs >5

train20 <- subset(train19,select=-hp)
model20 <- lm(price~.,train20)
summary(model20)
vif(model20)
```

To avoid excessively long lists of outputs, I have condensed the variable selection process in the code above. I first modeled using all 34 predictors and found that fuel_Diesel and body_Pickup.Truck are linearlly related. I removed the variables and modeled again.

Because there are no aliased variables, I inspected the VIFs (variance inflation factors) of each variable. Predictors with high VIFs have high multicollinearity, or correlation with other predictors. The predictor with the highest VIF is removed and the remaining variables are used to model again. This process is repeated until no variable has a VIF greater than 10.

Following this, the summary() output of each model is checked. The Pr(>|t|) column in particular is used to see whether the relationship between each predictor and the response is due to chance. The column represents the p-value, where for our chosen alpha value of 0.05, a p-value smaller than 0.05 suggests the relationship is not due to chance.

Using this, the procedure of inspecting and dropping the variable with the greatest p-value is performed repeatedly until satisfactory.

Finally, the VIF is inspected one last time. I decided to drop one more variable in an effort for a more parsimonous model. The resulting model has predictors with VIFs less than 5.

I now inspect whether this model satisfies the conditions for a linear regression model:

- the mean of the response is a linear function
- independent errors
- normally distributed errors
- the errors have equal variance

<br/>

### Diagnostic Plots of Linear Model

```{r}
par(mfrow = c(2,2))
plot(model20)
par(mfrow = c(1,1))
hist(resid(model20))
shapiro.test(resid(model20))
```

The above diagnostic plots give insight to the model's behavior. 

We can see that the model is satisfactory for the linearity and equal variance conditions from the residuals vs fitted plot. There is no clear pattern to the distribution of the residuals as they all seem to be randomly scattered around the 0 line. There is suggestion of outliers as well as slight clustering to the left.

Independence of the errors are assumed in this model, as the dates/locations of the observations are unknown. This means we cannot establish whether there is a temporal or spatial relation in our data.

The normal QQ plot shows that our errors are not completely normally distributed. The right tail of the plot deviates significantly from the diagonal line. This is further supported by the histogram of residuals and the results of the Shapiro-Wilk test. The histogram does not show a normal distribution and the low p-value rejects the null hypothesis of normally distributed residuals.


To remedy these problems, we will use the Box Cox method to appropriately transform the response variable.

<br/>

### Transforming Linear Model & Inspection

```{r}
library(MASS)
boxcox(model20)
tmodel <- lm(log(price)~.,train20)
par(mfrow = c(2,2))
plot(tmodel)
par(mfrow = c(1,1))
hist(resid(tmodel))
shapiro.test(resid(tmodel)) #actually normal!
summary(tmodel)
```

Based on the output of the Box Cox plot, we take lambda to be 0 -- meaning a log transformation of the price response variable.

Using the same diagnostic methods, we see that the conditions for linear regression are met. Residuals appear to be much more well behaved. While it appears the normality condition is not fully satisfied based on the QQ plot and histogram of residuals, the Shapiro Wilk test confirms the normality of our residuals.

<br/>

### Outliers and Influential Points

```{r}
#sum(hatvalues(tmodel)) # should match p = 14
#hatids <- which(abs(hatvalues(tmodel))>0.0645) #13 obs
#hatvalues(tmodel)[hatids]

#dfids <- which(dffits(tmodel)>0.30715) #11 obs
#dffits(tmodel)[dfids]

#sdrids <- which(abs(rstudent(tmodel))>3) #3 obs
#rstudent(tmodel)[sdrids]
#rstudent(tmodel)

#influenceIndexPlot(tmodel,id=TRUE)

cookids <- which(cooks.distance(tmodel)>1)
cookids1 <- which(cooks.distance(tmodel)>.5)
cooks.distance(tmodel)[cookids] #none!
cooks.distance(tmodel)[cookids1] #also none

```

This model contains 13 predictors, meaning p = 14 (counting intercept).
There are 651 observations making up our training set, making n = 651.
Therefore, the cutoffs for high leverage values are:
3*14/651 = 0.0645 (=3p/n)
or alternatively: 2*14/651 = 0.04301. (=2p/n) as stricter cutoff.

The cutoff for DFFITS is calculated as:
2*sqrt((p+1)/(n-p-1)) = 0.30715
DFFITS is another method of identifying high influence observations (outlying observations on the x axis).

The cutoff for Cook's distance is 0.5 and 1, along with any standout values. Cook's distance is yet another method for determining high influence data. There are no observations deemed as influential using this method in our model.

Cutoff for studentized deleted residuals is 3.
Observations  442, 500, and 544 have |t_i| > 3, and are considered to be outliers (outlying observations on the y axis).

Excessively long output have been shortened here. Observations 442, 500, and 544 will be removed due to their repeated appearance in these diagnoses and the model's performance will be tested again. It is important to note that observations should not be removed simply because they do not fit our preconcieved notions of how the data should be modeled.

<br/>

### Testing Model with Removed Points

```{r testing effects of removing points}
removed <- train20[-c(which(abs(rstudent(tmodel))>3)),]

rmodel <- lm(log(price)~.,removed)
summary(rmodel)
par(mfrow = c(2,2))
plot(rmodel)
par(mfrow = c(1,1))
hist(resid(rmodel))
shapiro.test(resid(rmodel))
```

We note that the p-value of the Shapiro-Wilk test and adjusted R-squared value increased, but other aspects of the model remain largely the same or even slightly worse. The variable drive_4X4 is deemed less significant than before, and new influential points/outliers seem to have manifested. The performances of the models will be compared later.

We currently have two 'complete' models: the base transformed model without removed points, and the same model fitted with points removed.

<br/>

### Forwards and Backwards Stepwise Regression Using AIC

```{r stepwise regression w/ AIC}
#forwards
mod0 = lm(price~1,train)
modup = lm(price~.,train)
#step(mod0,scope=list(lower=mod0,upper=modup))
fmod = lm(price ~ torque + miles + fuel_Flex.Fuel + hp + drive_AWD + 
    ftrackw + wheelb + rtrackw + drive_RWD + drive_4X4 + interior_Brown + 
    body_Coupe + width + body_Wagon + fuel_Gas.Electric.Hybrid + 
    tm_Manual + hmpg + fuel_Electric + cmpg + gclear + height + 
    body_Van.Minivan, data = train)

#backwards
#stepAIC(modup,direction="backward")
bmod = lm(price ~ miles + hp + torque + height + width + gclear + 
    wheelb + ftrackw + rtrackw + cmpg + hmpg + body_Coupe + body_Van.Minivan + 
    body_Wagon + interior_Brown + tm_Manual + drive_4X4 + drive_AWD + 
    drive_RWD + fuel_Electric + fuel_Flex.Fuel + fuel_Gas.Electric.Hybrid, 
    data = train)

summary(fmod)
```

An alternative method to linearly model used car data is used here. Again, due to excessively long outputs, some outputs have been surpressed.

Here, I have used stepwise regression in both forwards and backwards directions to select variables for use in modeling. Both forwards and backwards methods have resulted in the same model despite the surpression effects of the forward method. These methods choose the model with the lowest AIC.

It should be noted that all of the variables from our first model are contained here. 

<br/>

### Inspecting Stepwise Model

```{r testing fmod}
par(mfrow = c(2,2))
plot(fmod)
par(mfrow = c(1,1))
hist(resid(fmod))
shapiro.test(resid(fmod))
boxcox(fmod)
```

We note that this model does not satisfy the conditions of linear regression. The proper transformation of the response variable is determined to be logarithmic.

```{r transforming fmod}
tfmod = lm(log(price) ~ torque + miles + fuel_Flex.Fuel + hp + drive_AWD + 
    ftrackw + wheelb + rtrackw + drive_RWD + drive_4X4 + interior_Brown + 
    body_Coupe + width + body_Wagon + fuel_Gas.Electric.Hybrid + 
    tm_Manual + hmpg + fuel_Electric + cmpg + gclear + height + 
    body_Van.Minivan, data = train)

par(mfrow = c(2,2))
plot(tfmod)
par(mfrow = c(1,1))
hist(resid(tfmod))
shapiro.test(resid(tfmod))
```

The model is viable after transformation. It will be compared to the others later. We now have three models for comparison:

- transformed base model
- transformed base model fitted with points removed
- transformed stepwise model

<br/>

### Ridge & LASSO Regression

Ridge and lasso regression are shrinkage methods, with ridge shrinking coefficients towards zero and lasso being able to zero coefficients. This effectively increases our prediction accuracy and can decrease variance while maintaining or improving how interpretable the model is.

Both use a tuning parameter -- lambda. This is essentially a mathematical penalty for the models, where larger lambdas decrease variance without affecting the bias in ridge regression and can outright eliminate variables in lasso regression.

</br>

### LASSO Regression

```{r setup and lasso}
library(glmnet)
trainx <- data.matrix(subset(train,select=-price))
trainy <- train$price

testx <- data.matrix(subset(test,select=-price))
testy <- test$price


lambs <- seq(10,100000,10)
lasso_cv <- cv.glmnet(trainx,trainy,alpha=1,lambda=lambs,nfolds=10)
plot(lasso_cv)
#lasso_cv$lambda.min
#lasso_cv$lambda.1se
#choose lambda ~150
```

Ridge and lasso regression share the same glmnet function, with alpha = 1 indicating lasso regression is being performed.

The above plot displays the use of cross-validation, a resampling technique to estimate model performance. A range of potential lambda values are evaluated and plotted, giving a range of combinations of  lambda values, variables, and MSEs to choose from. We use 10-fold CV to select our lambda.

Though the preferred lambda value is 10-40, we opt for a lambda around 150 in pursuit of a more streamlined model at the expense of slightly higher MSE.


```{r lasso}
# Fit model on training set
lassomod <- glmnet(trainx,trainy,alpha=1,lambda=150)
coefficients(lassomod)
```

We see that the lasso regression has zeroed out some variables, effectively choosing variables as it was meant to do. Its performance will be compared to the other models later. The LASSO model is expected to behave similary to our ordinary least squares model and have better performance. An interesting fact to note is that when lambda is set to zero, the result is essentially our OLS model. The larger lambda we have selected, on the other hand, introduces enough shrinkage to mimic a best subset selection of variables.

<br/>

### Ridge Regression

```{r ridge}
ridge_cv <- cv.glmnet(trainx,trainy,alpha=0,lambda=lambs,nfolds=10)
plot(lasso_cv)
#ridge_cv$lambda.min
#ridge_cv$lambda.1se
#choose lambda ~400
```

Alpha is set to zero here in glmnet(), indicating ridge regression. The same 10-fold cross validation method is used to select our lambda parameter.
Lambda is taken to be 400 here to achieve a balance between performance and sparsity of variables in our model.

```{r ridge fit}
# Fit model on training set
ridgemod <- glmnet(trainx,trainy,alpha=0,lambda=400)
coefficients(ridgemod)
```

We note much more variables are present here than in lasso regression. Diesel and Pickup Truck are dropped because they are linearly related variables. Apart from them, no other variables have been dropped.

Now, there are a total of five models for comparison:

- transformed base model
- transformed base model fitted with points removed
- transformed stepwise model
- LASSO regression model
- ridge regression model

</br>

### Evaluation and Comparison of Linear Models

```{r eval lin models, echo=F}
#creating dataframe
results <- data.frame(model=character(),train.rmse=numeric(),test.rmse=numeric(),stringsAsFactors = FALSE)



# Original transformed model
tmodpredtr <- predict(tmodel,train) # Apply model on train set
tmodtr <- mean((train$price-exp(tmodpredtr))^2) # train mse
tmodpredte <- predict(tmodel,test) # Apply model on test set
tmodte <- mean((test$price-exp(tmodpredte))^2) # test mse
results[1,] <- c("base",round(tmodtr^.5,2),round(tmodte^.5,2))

# Transformed model w/ removed points
rmodpredtr <- predict(rmodel,train) # Apply model on train set
rmodtr <- mean((train$price-exp(rmodpredtr))^2) # train mse
rmodpredte <- predict(rmodel,test) # Apply model on test set
rmodte <- mean((test$price-exp(rmodpredte))^2) # test mse
results <- rbind(results,c("removed",round(rmodtr^.5,2),round(rmodte^.5,2)))

# Transformed stepwise model
tfmodpredtr <- predict(tfmod,train) # Apply model on train set
tfmodtr <- mean((train$price-exp(tfmodpredtr))^2) # train mse
tfmodpredte <- predict(tfmod,test) # Apply model on test set
tfmodte <- mean((test$price-exp(tfmodpredte))^2) # test mse
results <- rbind(results,c("stepwise",round(tfmodtr^.5,2),round(tfmodte^.5,2)))

#lasso
lassopredtr <- predict.glmnet(lassomod,trainx) # Apply model on train set
lfittr <- mean((train$price-lassopredtr)^2) # train mse
lassopredte <- predict.glmnet(lassomod,testx) # Apply model on test set
lfitte <- mean((test$price-lassopredte)^2) # test mse
results <- rbind(results,c("lasso",round(lfittr^.5,2),round(lfitte^.5,2)))

#ridge
ridgepredtr <- predict.glmnet(ridgemod,trainx) # Apply model on train set
rfittr <- mean((train$price-ridgepredtr)^2) # train mse
ridgepredte <- predict.glmnet(ridgemod,testx) # Apply model on test set
rfitte <- mean((test$price-ridgepredte)^2) # test mse
results <- rbind(results,c("ridge",round(rfittr^.5,2),round(rfitte^.5,2)))

colnames(results) <- c("model","train.rmse","test.rmse")

results
```

The RMSE is used as our method of inspection, as it gives the dollar value by which our models' are off by. The train column is the error in predicting values from our training observations, likewise for the test column for testing observations. The test column is of particular interest because the model has never 'seen' these observations, giving a good measure of the real-world performance of our models. But more importantly, the differences in the two columns give insight to the variance-bias tradeoff in our models.

In our base model, we see that both training and test RMSEs are comparatively high to other models. The similar values suggest that our model leans towards high bias and low variance, which is expected for this OLS linear model.

We first note that by removing data values, our base model's performance actually decreased. This indicates that for future modeling and consideration, those observations should not be removed. 

The stepwise model contains much more variables (22) than the 13 predictors used in our base model. From the larger difference between train and test RMSEs, we can assume this model leans towards having lower bias but higher variance. This is expected of having a more complex model.

The LASSO model also contains a large number of variables (21). This can be changed easily by using a higher lambda value. We should note the larger difference between train and test RMSEs, indicating a relative middle ground in model complexity, bias, and variance. In our case, however, LASSO performs slightly worse than the stepwise model. 

The ridge regression model has the same trend seen in the stepwise model, with a larger difference in train and test RMSEs. This model also has the worst performance, suggesting the presence of overfitting when considering the model's complexity.

It should be noted that relationships are rarely strictly linear, so there is almost certainly underlying bias in all of these models. Overfitting could also have resulted, as seen in our models with high numbers of predictors -- enforcing internal validation bias.

<br/>

### Regression Tree

Decision trees are not as powerful as some of the techniques used in this project. They do provide some of the best insights and interpretation available for our data. Because of their nature, they have high variance and low bias.

```{r reg tree}
#reg tree
set.seed(1)
library(rpart)
library(rpart.plot)
treefit <- rpart(price~.,train,method="anova")
rpart.plot(treefit)
printcp(treefit)
treefit$variable.importance
plotcp(treefit)
```

We see from the first split that the tree considers torque as the most important variable in determining car price. From the variable importance output, the most important variables are torque, followed by horsepower, city mpg, etc. The variables actually used are cmpg, gclear, hp, miles, torque, and year.

From the complexity parameter plot, there appears to be diminishing returns around 8 nodes, noting the relative error and complexity parameter moving forward.

We choose a complexity parameter of 0.021085 to prune the tree.

```{r reg tree cont}
treefit2 <- rpart(price~.,train,method="anova", cp=0.021085)
rpart.plot(treefit2)

printcp(treefit2)
treefit2$variable.importance
plotcp(treefit2)
```

The same variables used in the full tree are used here.
We will compare these two trees later, and determine whether the prune was beneficial.

<br/>

### Ensemble Methods

The following techniques use multiple trees, similar to the one created earlier to improve predictive performance.

<br/>

### Bagging
```{r Bagging}
set.seed(1)
library(randomForest)
bagfit <- randomForest(price~.,data=train,mtry=34,importance=TRUE,ntrees=500)
bagfit
plot(bagfit)
importance(bagfit)
```

Decision trees by themselves suffer from high variance. Bagging is bootstrap aggregation, which serves to lower this variance by repeatedly sampling from our training dataset and averaging model predictions.

In bagging, m=p, the number of predictors.

The %IncMSE column in the above output gives insight to the importance of various predictor variables, with higher values being more important.

<br/>

### Random Forests
```{r Random Forests, echo = FALSE}
set.seed(1)

forestfit <- randomForest(price~.,data=train,mtry=34/3, ntree=500,importance=TRUE)

forestfit
plot(forestfit)
importance(forestfit)
```

Random forests are very similar to bagged trees, with similar methodology. The difference is in decorrelating trees by randomly sampling a subset of our predictors for consideration at node splits. Bagging considers all predictors as candidates for a node split. This difference serves to differentiate the trees in the forest, and gives other predictors a larger 'voice'.

In random forests, m=p/3 for regression purposes.

<br/>

### Boosting

Boosting follows the methodology of growing multiple trees, except boosting grows trees in a sequence -- each tree is based on information from older trees. Boosting involves a shrinkage parameter that causes the model to learn more slowly, assigning weights to observations.

```{r boosting}
library(gbm)
set.seed(1)

trees <- seq(500,10000,500)
shrinks <- seq(0.05,0.5,0.01)
depths <- seq(1,10,1)
trainerrors <- c()
testerrors <- c()

#for (t in trees) {
#  tempboost <- gbm(price~.,data=train,distribution="gaussian",n.trees=t)
#  boostftrp <- predict(tempboost, newdata = train, n.trees=t)
#  boostftrmse <- mean((train$price-boostftrp)^2) # test mse
#  boostftep <- predict(tempboost, newdata = test, n.trees=t)
#  boostftemse <- mean((test$price-boostftep)^2) # test mse
#  trainerrors <- rbind(trainerrors,boostftrmse)
#  testerrors <- rbind(testerrors,boostftemse)
#} #4000 trees 

#for (s in shrinks) {
#  tempboost <- gbm(price~.,data=train,distribution="gaussian",n.trees=4000,shrinkage = s)
#  boostftrp <- predict(tempboost, newdata = train, n.trees=4000)
#  boostftrmse <- mean((train$price-boostftrp)^2) # test mse
#  boostftep <- predict(tempboost, newdata = test, n.trees=4000)
#  boostftemse <- mean((test$price-boostftep)^2) # test mse
#  trainerrors <- rbind(trainerrors,boostftrmse)
#  testerrors <- rbind(testerrors,boostftemse)
#} #best shrinkage is 0.08

#for (d in depths) {
#  tempboost <- gbm(price~.,data=train,distribution="gaussian",n.trees=4000,shrinkage = 0.08,interaction.depth=d)
#  boostftrp <- predict(tempboost, newdata = train, n.trees=4000)
#  boostftrmse <- mean((train$price-boostftrp)^2) # test mse
#  boostftep <- predict(tempboost, newdata = test, n.trees=4000)
#  boostftemse <- mean((test$price-boostftep)^2) # test mse
#  trainerrors <- rbind(trainerrors,boostftrmse)
#  testerrors <- rbind(testerrors,boostftemse)
#} #best depth is 4

#testerrors
#max(depths[testerrors == min(testerrors)])
#trainerrors

boostfit <- gbm(price~.,data=train,distribution="gaussian",n.trees=4000,shrinkage=0.08,interaction.depth=4)
```
Cross validation is used to tune the multiple parameters used in boosting. We first select the best number of trees to use to prevent overfitting from too many trees. The shrinkage controls learning speed, and is selected next. The complexity of the ensemble is the last parameter to be selected, controlling for the number of variable interactions.

From selection, we find that 4000 trees, lambda = 0.08, and a depth of 4 to be optimal for our data.

The distribution is set to Gaussian here because we are performing regression. It would be set to Bernoulli if we were classifying.

</br>

### Evaluation and Comparison of Tree Based Models 

```{r eval tree models, echo=F}
#creating dataframe
treeresults <- data.frame(model=character(),train.rmse=numeric(),test.rmse=numeric(),stringsAsFactors = FALSE)

# Original Regression Tree
tfitpredtr <- predict(treefit,train) # Apply model on train set
treetr <- mean((train$price-tfitpredtr)^2) # train mse
tfitpredte <- predict(treefit,test) # Apply model on test set
treete <- mean((test$price-tfitpredte)^2) # test mse
treeresults[1,] <- c("reg. tree",round(treetr^.5,2),round(treete^.5,2))

# Pruned Regression Tree
tfit2predtr <- predict(treefit2,train) # Apply model on train set
tree2tr <- mean((train$price-tfit2predtr)^2) # train mse
tfit2predte <- predict(treefit2,test) # Apply model on test set
tree2te <- mean((test$price-tfit2predte)^2) # test mse
treeresults <- rbind(treeresults,c("pruned tree",round(tree2tr^.5,2),round(tree2te^.5,2)))

# Bagging
bftrp <- predict(bagfit,newdata=train)
bftrmse <- mean((train$price - bftrp)^2)
bftep <- predict(bagfit,newdata=test)
bftemse <- mean((test$price - bftep)^2)
treeresults <- rbind(treeresults,c("bagging",round(bftrmse^.5,2),round(bftemse^.5,2)))

# Random Forest
rftrp <- predict(forestfit, newdata = train)
rftrmse <- mean((train$price-rftrp)^2) # test mse
rftep <- predict(forestfit, newdata = test)
rftemse <- mean((test$price-rftep)^2) # test mse
treeresults <- rbind(treeresults,c("rand forest",round(rftrmse^.5,2),round(rftemse^.5,2)))


# Boosting
boostftrp <- predict(boostfit, newdata = train, n.trees=4000)
boostftrmse <- mean((train$price-boostftrp)^2) # test mse
boostftep <- predict(boostfit, newdata = test, n.trees=4000)
boostftemse <- mean((test$price-boostftep)^2) # test mse
treeresults <- rbind(treeresults,c("boosting",round(boostftrmse^.5,2),round(boostftemse^.5,2)))


colnames(treeresults) <- c("model","train.rmse","test.rmse")

treeresults
```

The RMSE of both train and test sets are used again to determine model performance, giving the dollar amount by which our models deviate. 

Both decision trees offer better performance than our linear models. This suggests that a nonlinear relationship is present.
We note that the pruned tree has slightly better test data performance, implying a better model despite its higher train RMSE.

The ensemble methods have greatly increased performance, with all models showing half of the deviance of nonensemble models. They have lower variance but slightly higher bias as a tradeoff. These methods have the strongest predictive power, but low explainability due to their inherent complexity.

Bagged trees and our random forest have similar performance due to the similarities in their methodology. The random forest has the best performance out of all models here. This implies some of the features we found to be important may be overstated in other models.

The boosting model performed second best. There seems to be overfitting present in our model, as the train RMSE is the lowest of all models by far.
With more dedicated tuning using hyperplanes, this can be resolved. Boosting is able to reduce both bias and variance, while bagging and random forests have biases locked to a single tree's.

It should be noted that the nonensemble methods provide better explaining power at the cost of predictive power, and that ensemble methods provide the reverse. Ensemble methods also require much more computation power/time.

</br>

### All Model Results

The RMSEs of all models used in this analysis are shown here for comparison.

```{r eval all models, echo=F}
#creating dataframe
finresults <- data.frame(model=character(),train.rmse=numeric(),test.rmse=numeric(),stringsAsFactors = FALSE)

# Original transformed model
tmodpredtr <- predict(tmodel,train) # Apply model on train set
tmodtr <- mean((train$price-exp(tmodpredtr))^2) # train mse
tmodpredte <- predict(tmodel,test) # Apply model on test set
tmodte <- mean((test$price-exp(tmodpredte))^2) # test mse
finresults[1,] <- c("base",round(tmodtr^.5,2),round(tmodte^.5,2))

# Transformed model w/ removed points
rmodpredtr <- predict(rmodel,train) # Apply model on train set
rmodtr <- mean((train$price-exp(rmodpredtr))^2) # train mse
rmodpredte <- predict(rmodel,test) # Apply model on test set
rmodte <- mean((test$price-exp(rmodpredte))^2) # test mse
finresults <- rbind(finresults,c("removed",round(rmodtr^.5,2),round(rmodte^.5,2)))

# Transformed stepwise model
tfmodpredtr <- predict(tfmod,train) # Apply model on train set
tfmodtr <- mean((train$price-exp(tfmodpredtr))^2) # train mse
tfmodpredte <- predict(tfmod,test) # Apply model on test set
tfmodte <- mean((test$price-exp(tfmodpredte))^2) # test mse
finresults <- rbind(finresults,c("stepwise",round(tfmodtr^.5,2),round(tfmodte^.5,2)))

#lasso
lassopredtr <- predict.glmnet(lassomod,trainx) # Apply model on train set
lfittr <- mean((train$price-lassopredtr)^2) # train mse
lassopredte <- predict.glmnet(lassomod,testx) # Apply model on test set
lfitte <- mean((test$price-lassopredte)^2) # test mse
finresults <- rbind(finresults,c("lasso",round(lfittr^.5,2),round(lfitte^.5,2)))

#ridge
ridgepredtr <- predict.glmnet(ridgemod,trainx) # Apply model on train set
rfittr <- mean((train$price-ridgepredtr)^2) # train mse
ridgepredte <- predict.glmnet(ridgemod,testx) # Apply model on test set
rfitte <- mean((test$price-ridgepredte)^2) # test mse
finresults <- rbind(finresults,c("ridge",round(rfittr^.5,2),round(rfitte^.5,2)))

# Original Regression Tree
tfitpredtr <- predict(treefit,train) # Apply model on train set
treetr <- mean((train$price-tfitpredtr)^2) # train mse
tfitpredte <- predict(treefit,test) # Apply model on test set
treete <- mean((test$price-tfitpredte)^2) # test mse
finresults <- rbind(finresults,c("reg. tree", round(treetr^.5,2) ,round(treete^.5,2)))

# Pruned Regression Tree
tfit2predtr <- predict(treefit2,train) # Apply model on train set
tree2tr <- mean((train$price-tfit2predtr)^2) # train mse
tfit2predte <- predict(treefit2,test) # Apply model on test set
tree2te <- mean((test$price-tfit2predte)^2) # test mse
finresults <- rbind(finresults,c("pruned tree",round(tree2tr^.5,2),round(tree2te^.5,2)))

# Bagging
bftrp <- predict(bagfit,newdata=train)
bftrmse <- mean((train$price - bftrp)^2)
bftep <- predict(bagfit,newdata=test)
bftemse <- mean((test$price - bftep)^2)
finresults <- rbind(finresults,c("bagging",round(bftrmse^.5,2),round(bftemse^.5,2)))

# Random Forest
rftrp <- predict(forestfit, newdata = train)
rftrmse <- mean((train$price-rftrp)^2) # test mse
rftep <- predict(forestfit, newdata = test)
rftemse <- mean((test$price-rftep)^2) # test mse
finresults <- rbind(finresults,c("rand forest",round(rftrmse^.5,2),round(rftemse^.5,2)))


# Boosting
boostftrp <- predict(boostfit, newdata = train, n.trees=4000)
boostftrmse <- mean((train$price-boostftrp)^2) # test mse
boostftep <- predict(boostfit, newdata = test, n.trees=4000)
boostftemse <- mean((test$price-boostftep)^2) # test mse
finresults <- rbind(finresults,c("boosting",round(boostftrmse^.5,2),round(boostftemse^.5,2)))

colnames(finresults) <- c("model","train.rmse","test.rmse")

finresults
```

